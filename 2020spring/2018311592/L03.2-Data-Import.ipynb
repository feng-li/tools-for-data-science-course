{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and storage\n",
    "Input and output typically falls into a few main categories: reading text files and other more efficient on-disk formats like pdf and docx files, interacting with net-work sources like web APIs, and loading data from databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing a text file\n",
    "### Using `open()` to read and write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`open()` returns a file object, and is most commonly used with two arguments: `open(filename, mode)`. It is good practice to use the with keyword when dealing with file objects. The advantage is that the file is properly closed after its suite finishes.\n",
    "\n",
    "- `mode` can be 'r' for read\n",
    "- 'w' for only writing\n",
    "- 'a' opens the file for appending\n",
    "- 'r+' opens the file for both reading and writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\n",
      "    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\n",
      "    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\n",
      "    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\n",
      "    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/BABAnews.txt\", mode = 'r') as myfile: \n",
    "        print(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re not using the with keyword, then you should call f.close() to close the file and immediately free up any system resources used by it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\n",
      "    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\n",
      "    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\n",
      "    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\n",
      "    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "text = myfile.read()\n",
    "myfile.close()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f51008fa62ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the contents of a file, we use `f.read(size)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\\n    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\\n    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\\n    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\\n    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f.readline()` reads a single line from the file; a newline character (\\n) is left at the end of the string, and is only omitted on the last line of the file if the file doesn’t end in a newline. This makes the return value unambiguous; if `f.readline()` returns an empty string, the end of the file has been reached, while a blank line is represented by '\\n', a string containing only a single newline. \n",
    "\n",
    "If you want to read all the lines of a file in a list you can also use `list(f)` or `f.readlines()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    从2009年淘宝商城的促销活动，到2016年天猫商城全天交易额超越1207亿，“双十一”经过八年的升温，已经逐渐成长能够影响世界的中国电子商务行业年度盛事。在风光的“双十一”背后，技术保障团队又有什么样鲜为人知的故事呢？4月12日，阿里巴巴技术团队在京召开了分享会，独家揭秘“双十一”背后的“故事”，同时向大家推荐了题为《尽在双11——阿里巴巴技术演进与超越》的互联网丛书。\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile = open(\"data/BABAnews.txt\", mode = 'r')\n",
    "myfile.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    作为迄今为止，阿里巴巴集团唯一官方出品、全面阐述双 11 八年以来在技术和商业上演进和创新历程的书籍，《尽在双11——阿里巴巴技术演进与超越》由电子工业出版社博文视点与阿里巴巴联合出版。本书从五个方面全面精炼生动地进行剖析，揭秘世界奇迹双11背后的技术演进与创新，内容涵盖了双 11 背景下阿里技术架构八年来的演进，如何确保稳定性这条双 11 生命线的安全和可靠，技术和商业交织发展的历程，无线和互动的持续创新与突破，以及对商家的赋能和生态的促进与繁荣。\\n',\n",
       " '    “1秒钟同时完成12万笔支付、单日交易额定格在1207亿元、每秒同时创建17.5万笔订单……”在这一串惊人的数字背后，蕴藏着技术团队鲜为人知的故事，对于他们来说，每年双11最紧张的就是午夜零点难以想象的流量。会议现场，双十一技术团部大队长陈琴、双十一稳定性负责人丁宇、认知计算实验室负责人袁泉分别从各自负责的业务板块，向大家细致解读了“双十一”背后，技术团队所付出的艰辛。三位负责人纷纷表示，他们分享的内容都包含在《尽在双11——阿里巴巴技术演进与超越》一书中，是双11八年成长经验与技术创新的总结，也是阿里成长中摸索出的方法和方向的汇聚，更是诸多技术同学与技术大神的倾囊分享。\\n',\n",
       " '    阿里巴巴集团CEO张勇，对该书给予了高度的肯定。他认为，“《尽在双11——阿里巴巴技术演进与超越》以双11为着眼点，从技术的角度，展示了阿里巴巴的演进、变革与发展，系统地阐述了阿里巴巴重要阶段的技术进步历程。进无止境，我们希望将我们的经验分享给更多人，并希望与大家一起共同探索未来。”\\n',\n",
       " '    阿里巴巴集团CTO张剑锋也力荐《尽在双11——阿里巴巴技术演进与超越》一书。他说，“《尽在双11——阿里巴巴技术演进与超越》是对“双11”技术演进客观、翔实的还原，迄今为止还没有类似的其他书。”（周靖杰）\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Both `read()` and `readlines()` come with the concept of a cursor. After either command is executed, the cursor moves to the end of the file, leaving nothing more to read in. Therefore, once a file content has been read in, another attempt to read from the file object will produce an empty data object. If for some reason you must read the file content again, you must close and re-open the file. \n",
    "\n",
    "Lastly, rather than loading the entire file content into memory, you can iterate through the file object line by line using the `for` loop. This method is more memory-efficient and therefore recommended when dealing with a very large file. read()和readlines()都带有游标的概念。在执行任何一个命令之后，光标都会移动到文件的末尾，不再留下任何要读入的内容。因此，一旦文件内容被读入，对file对象的另一次读取尝试将产生一个空数据对象。如果由于某种原因必须再次读取文件内容，则必须关闭并重新打开文件。\n",
    "最后，您可以使用for循环逐行遍历文件对象，而不是将整个文件内容加载到内存中。这种方法的内存效率更高，因此在处理非常大的文件时推荐使用这种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,Class,Sex,Age,Survived,Freq\n",
      "1,1st,Male,Child,No,0\n",
      "2,2nd,Male,Child,No,0\n",
      "3,3rd,Male,Child,No,35\n",
      "4,Crew,Male,Child,No,0\n",
      "5,1st,Female,Child,No,0\n",
      "6,2nd,Female,Child,No,0\n",
      "7,3rd,Female,Child,No,17\n",
      "8,Crew,Female,Child,No,0\n",
      "9,1st,Male,Adult,No,118\n",
      "10,2nd,Male,Adult,No,154\n",
      "11,3rd,Male,Adult,No,387\n",
      "12,Crew,Male,Adult,No,670\n",
      "13,1st,Female,Adult,No,4\n",
      "14,2nd,Female,Adult,No,13\n",
      "15,3rd,Female,Adult,No,89\n",
      "16,Crew,Female,Adult,No,3\n",
      "17,1st,Male,Child,Yes,5\n",
      "18,2nd,Male,Child,Yes,11\n",
      "19,3rd,Male,Child,Yes,13\n",
      "20,Crew,Male,Child,Yes,0\n",
      "21,1st,Female,Child,Yes,1\n",
      "22,2nd,Female,Child,Yes,13\n",
      "23,3rd,Female,Child,Yes,14\n",
      "24,Crew,Female,Child,Yes,0\n",
      "25,1st,Male,Adult,Yes,57\n",
      "26,2nd,Male,Adult,Yes,14\n",
      "27,3rd,Male,Adult,Yes,75\n",
      "28,Crew,Male,Adult,Yes,192\n",
      "29,1st,Female,Adult,Yes,140\n",
      "30,2nd,Female,Adult,Yes,80\n",
      "31,3rd,Female,Adult,Yes,76\n",
      "32,Crew,Female,Adult,Yes,20"
     ]
    }
   ],
   "source": [
    "with open('data/Titanic.csv', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        print(line, end ='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing methods also come in a pair: `write()` and `writelines()`. Like the corresponding reading methods, `write()` handles a single string, while `writelines()` handles a list of strings. \n",
    "\n",
    "Below, `write()` writes a single string each time to the designated output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/testfile.txt', 'w') \n",
    "file.write('Hello World \\n') \n",
    "file.write('and this is another line.') \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This time, we have `tobuy`, a list of strings, which `writelines()` writes out at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tobuy = ['milk\\n', 'butter\\n', 'coffee beans\\n', 'arugula\\n']\n",
    "file = open('data/grocerylist.txt', 'w')\n",
    "file.writelines(tobuy) # writelines(list)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only the string type can be written.** Writing methods only works with strings: `write()` takes a single string, and `writelines()` takes a list which contains strings only. Non-string type data must be first coerced into the string type by using the `str()` function. 非字符串类型的数据必须首先通过使用str()函数强制转换为字符串类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7980b2b9ef32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/math.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pi's value is \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# trying to write float, doesn't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not float"
     ]
    }
   ],
   "source": [
    "pi = 3.141592\n",
    "fout = open('data/math.txt', 'w')\n",
    "fout.write(\"Pi's value is \")\n",
    "fout.write(pi) # trying to write float, doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout.write(str(pi))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to read text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use other modules to read text files. For example, we can use **numpy** to read *txt* file.  **csv** and **pandas** can be used to read *csv* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    1.  200.1]\n",
      " [  2.    2.  199.5]\n",
      " [  3.    3.  199.4]\n",
      " [  4.    4.  198.9]\n",
      " [  5.    5.  199. ]\n",
      " [  6.    6.  200.2]\n",
      " [  7.    7.  198.6]\n",
      " [  8.    8.  200. ]\n",
      " [  9.    9.  200.3]\n",
      " [ 10.   10.  201.2]\n",
      " [ 11.   11.  201.6]\n",
      " [ 12.   12.  201.5]\n",
      " [ 13.   13.  201.5]\n",
      " [ 14.   14.  203.5]\n",
      " [ 15.   15.  204.9]\n",
      " [ 16.   16.  207.1]\n",
      " [ 17.   17.  210.5]\n",
      " [ 18.   18.  210.5]\n",
      " [ 19.   19.  209.8]\n",
      " [ 20.   20.  208.8]\n",
      " [ 21.   21.  209.5]\n",
      " [ 22.   22.  213.2]\n",
      " [ 23.   23.  213.7]\n",
      " [ 24.   24.  215.1]\n",
      " [ 25.   25.  218.7]\n",
      " [ 26.   26.  219.8]\n",
      " [ 27.   27.  220.5]\n",
      " [ 28.   28.  223.8]\n",
      " [ 29.   29.  222.8]\n",
      " [ 30.   30.  223.8]\n",
      " [ 31.   31.  221.7]\n",
      " [ 32.   32.  222.3]\n",
      " [ 33.   33.  220.8]\n",
      " [ 34.   34.  219.4]\n",
      " [ 35.   35.  220.1]\n",
      " [ 36.   36.  220.6]\n",
      " [ 37.   37.  218.9]\n",
      " [ 38.   38.  217.8]\n",
      " [ 39.   39.  217.7]\n",
      " [ 40.   40.  215. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "data = np.loadtxt('data/BJsales.txt', skiprows = 1, delimiter='\\t')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID  Class  Sex  Age  Survived  Freq\n",
      "1  1st  Male  Child  No  0\n",
      "2  2nd  Male  Child  No  0\n",
      "3  3rd  Male  Child  No  35\n",
      "4  Crew  Male  Child  No  0\n",
      "5  1st  Female  Child  No  0\n",
      "6  2nd  Female  Child  No  0\n",
      "7  3rd  Female  Child  No  17\n",
      "8  Crew  Female  Child  No  0\n",
      "9  1st  Male  Adult  No  118\n",
      "10  2nd  Male  Adult  No  154\n",
      "11  3rd  Male  Adult  No  387\n",
      "12  Crew  Male  Adult  No  670\n",
      "13  1st  Female  Adult  No  4\n",
      "14  2nd  Female  Adult  No  13\n",
      "15  3rd  Female  Adult  No  89\n",
      "16  Crew  Female  Adult  No  3\n",
      "17  1st  Male  Child  Yes  5\n",
      "18  2nd  Male  Child  Yes  11\n",
      "19  3rd  Male  Child  Yes  13\n",
      "20  Crew  Male  Child  Yes  0\n",
      "21  1st  Female  Child  Yes  1\n",
      "22  2nd  Female  Child  Yes  13\n",
      "23  3rd  Female  Child  Yes  14\n",
      "24  Crew  Female  Child  Yes  0\n",
      "25  1st  Male  Adult  Yes  57\n",
      "26  2nd  Male  Adult  Yes  14\n",
      "27  3rd  Male  Adult  Yes  75\n",
      "28  Crew  Male  Adult  Yes  192\n",
      "29  1st  Female  Adult  Yes  140\n",
      "30  2nd  Female  Adult  Yes  80\n",
      "31  3rd  Female  Adult  Yes  76\n",
      "32  Crew  Female  Adult  Yes  20\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('data/Titanic.csv') as csvfile:\n",
    "    titanicReader = csv.reader(csvfile)\n",
    "    for row in titanicReader:\n",
    "        print('  '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` I/O API is a set of top level `reader` functions accessed like `read_csv()` that generally return a pandas object. These functions includes\n",
    "\n",
    "    read_excel\n",
    "    read_hdf\n",
    "    read_sql\n",
    "    read_json\n",
    "    read_msgpack (experimental)\n",
    "    read_html\n",
    "    read_gbq (experimental)\n",
    "    read_stata\n",
    "    read_sas\n",
    "    read_clipboard\n",
    "    read_pickle\n",
    "    \n",
    "See [pandas IO tools](http://pandas.pydata.org/pandas-docs/stable/io.html) for detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Class     Sex    Age Survived  Freq\n",
       "0   1   1st    Male  Child       No     0\n",
       "1   2   2nd    Male  Child       No     0\n",
       "2   3   3rd    Male  Child       No    35\n",
       "3   4  Crew    Male  Child       No     0\n",
       "4   5   1st  Female  Child       No     0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanicData = pd.read_csv('data/Titanic.csv') \n",
    "titanicData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have used `read_table()` and specifying the delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1st</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Crew</td>\n",
       "      <td>Male</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1st</td>\n",
       "      <td>Female</td>\n",
       "      <td>Child</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Class     Sex    Age Survived  Freq\n",
       "0   1   1st    Male  Child       No     0\n",
       "1   2   2nd    Male  Child       No     0\n",
       "2   3   3rd    Male  Child       No    35\n",
       "3   4  Crew    Male  Child       No     0\n",
       "4   5   1st  Female  Child       No     0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanicData = pd.read_table('data/Titanic.csv', sep=',')\n",
    "titanicData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file will not always have a header row. To read this in, you have a couple of options. You can allow pandas to assign default column names, or you can specify names yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>199.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>198.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1      2\n",
       "0  1  1  200.1\n",
       "1  2  2  199.5\n",
       "2  3  3  199.4\n",
       "3  4  4  198.9\n",
       "4  5  5  199.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('data/BJsales.csv', header = None)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>199.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>198.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Time  Value\n",
       "0   1     1  200.1\n",
       "1   2     2  199.5\n",
       "2   3     3  199.4\n",
       "3   4     4  198.9\n",
       "4   5     5  199.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('data/BJsales.csv', names=['ID', 'Time', 'Value'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be exported to delimited format. Let’s consider one of the CSV files read above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_csv('data/sales.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Microsoft Excel file using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel files are a huge part of any business operation and it becomes imperative that you learn exactly how to import these into python for data analysis. In order to do this we can use the code snippet shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HairEyeColor', 'Nile']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Eye</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Red</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blond</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Black</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Male</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Hair    Eye   Sex  Freq\n",
       "0   1  Black  Brown  Male    32\n",
       "1   2  Brown  Brown  Male    53\n",
       "2   3    Red  Brown  Male    10\n",
       "3   4  Blond  Brown  Male     3\n",
       "4   5  Black   Blue  Male    11"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = 'data/example.xlsx'\n",
    "myfile = pd.ExcelFile(file) \n",
    "print(myfile.sheet_names) #printing out all the sheet names in the excel file\n",
    "dataframe = myfile.parse('HairEyeColor') #extracting data from the first sheet as a dataframe\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a pdf file using `PyPDF2`\n",
    "\n",
    "`PyPDF2` is a python library built as a PDF toolkit. It is capable of:\n",
    "\n",
    "- Extracting document information (title, author, …)\n",
    "- Splitting documents page by page\n",
    "- Merging documents page by page\n",
    "- Cropping pages\n",
    "- Merging multiple pages into a single page\n",
    "- Encrypting and decrypting PDF files\n",
    "- and more!\n",
    "\n",
    "Here we only demonstrate how to read a pdf file. Please find more details on https://pypi.org/project/PyPDF2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 10 kB/s eta 0:00:0146\n",
      "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61084 sha256=983324fadc91955713433bf4732b2b93d7da29dfed54b31239971cf8735182fe\n",
      "  Stored in directory: /Users/yangzhiwei/Library/Caches/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELINGCONDITIONALDENSITIESUSINGFINITESMOOTH\n",
      "MIXTURES\n",
      "FENGLI,MATTIASVILLANI,ANDROBERTKOHN\n",
      "SverigesRiksbankWorkingPaperSeriesNo.245\n",
      "August2010\n",
      "Abstract.\n",
      "Smoothmixtures,i.e.mixturemodelswithcovariate-depen\n",
      "dentmixingweights,\n",
      "areveryusefulexiblemodelsforconditionaldensities.P\n",
      "reviousworkshowsthatusingtoo\n",
      "simplemixturecomponentsformodelingheteroscedastican\n",
      "d/orheavytaileddatacangive\n",
      "apoort,evenwithalargenumberofcomponents.Thispapere\n",
      "xploreshowwellasmooth\n",
      "mixtureofsymmetriccomponentscancaptureskeweddata.Si\n",
      "mulationsandapplicationson\n",
      "realdatashowthatincludingcovariate-dependentskewnes\n",
      "sinthecomponentscanleadto\n",
      "substantiallyimprovedperformanceonskeweddata,oftenu\n",
      "singamuchsmallernumberof\n",
      "components.Furthermore,variableselectioniseectivei\n",
      "nremovingunnecessarycovariatesin\n",
      "theskewness,whichmeansthatthereislittlelossinallowi\n",
      "ngforskewnessinthecomponents\n",
      "whenthedataareactuallysymmetric.Wealsointroducesmoo\n",
      "thmixturesofgammaand\n",
      "log-normalcomponentstomodelpositively-valuedrespons\n",
      "evariables.\n",
      "Keywords\n",
      ":Bayesianinference,MarkovchainMonteCarlo,MixtureofE\n",
      "xperts,Variable\n",
      "selection\n",
      "Li:DepartmentofStatistics,StockholmUniversity,SE-10\n",
      "691Stockholm,Sweden.Villani:Research\n",
      "Division,SverigesRiksbankandDepartmentofStatistics,\n",
      "StockholmUniversity.Kohn:AustralianSchool\n",
      "ofBusiness,UniversityofNewSouthWales,UNSW,Sydney205\n",
      "2,Australia.Theviewsexpressedinthis\n",
      "paperaresolelytheresponsibilityoftheauthorsandshoul\n",
      "dnotbeinterpretedasreectingtheviewsofthe\n",
      "ExecutiveBoardofSverigesRiksbank.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdfFileObj = open('data/Li2011Wiley.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "pdfReader.numPages\n",
    "pageObj = pdfReader.getPage(0)\n",
    "print(pageObj.extractText())\n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a word file using `docx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/8e/5a01644697b03016de339ef444cfff28367f92984dc74eddaab1ed60eada/docx-0.2.4.tar.gz\n",
      "Collecting lxml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/85/8b1a9a33b451114e352c29a200320f086ea30ff163599dd39cb627e7c86a/lxml-4.5.0-cp37-cp37m-macosx_10_9_x86_64.whl (4.5MB)\n",
      "\u001b[K     |▊                               | 102kB 3.4kB/s eta 0:21:55\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 382, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 201, in resolve\n",
      "    self._resolve_one(requirement_set, req)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 365, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 313, in _get_abstract_dist_for\n",
      "    req, self.session, self.finder, self.require_hashes\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 194, in prepare_linked_requirement\n",
      "    progress_bar=self.progress_bar\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 465, in unpack_url\n",
      "    progress_bar=progress_bar\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 316, in unpack_http_url\n",
      "    progress_bar)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 551, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 253, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/utils/hashes.py\", line 80, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 223, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_internal/download.py\", line 212, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.10.tar.gz (5.5 MB)\n",
      "\u001b[K     |███████▏                        | 1.2 MB 2.2 kB/s eta 0:32:07\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 313, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1840, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\", line 1646, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/http/client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/http/client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 328, in recv_into\n",
      "    return self.recv_into(*args, **kwargs)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py\", line 326, in recv_into\n",
      "    raise timeout(\"The read operation timed out\")\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/commands/install.py\", line 331, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 177, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 333, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 482, in prepare_linked_requirement\n",
      "    hashes=hashes,\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 287, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 159, in unpack_http_url\n",
      "    link, downloader, temp_dir.path, hashes\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/operations/prepare.py\", line 303, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/utils/ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_internal/network/utils.py\", line 39, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/yangzhiwei/opt/anaconda3/lib/python3.7/site-packages/pip/_vendor/urllib3/response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ab6a63d52679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Li2011Wiley.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/docx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mTAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPendingDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "doc = docx.Document('data/Li2011Wiley.docx')\n",
    "print(len(doc.paragraphs))\n",
    "print(doc.paragraphs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with HTML and Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many websites have public APIs providing data feeds via JSON or some other format. There are a number of ways to access these APIs from Python; one easy-to-use method that I recommend is the requests package (http://docs.python-requests.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-96c0fa68d825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gb18030'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://sp0.baidu.com/8aQDcjqpAAV3otqbppnN2DJv/api.php?resource_id=6899&query=失信执行人名单&iname=中国银行'\n",
    "resp = requests.get(url)\n",
    "import json\n",
    "data = json.loads(resp.text, encoding='gb18030')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71c3ab347e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcaseCode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caseCode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mareaName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'areaName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "result = data['data'][0]['result']\n",
    "for i in range(len(result)):\n",
    "        data = result[i]\n",
    "        caseCode = data['caseCode']\n",
    "        areaName = data['areaName']\n",
    "        businessEntity = data['businessEntity']\n",
    "        courtName = data['courtName']\n",
    "        duty = data['duty']\n",
    "        performance = data['performance']\n",
    "        disruptTypeName = data['disruptTypeName']\n",
    "        publishDate = data['publishDate']\n",
    "        regDate = data['regDate']\n",
    "        gistId = data['gistId']\n",
    "        gistUnit = data['gistUnit']\n",
    "        cardNum = data['cardNum']\n",
    "        print('  '.join([caseCode, areaName, businessEntity, courtName, duty, performance, disruptTypeName, publishDate, regDate, gistId, gistUnit, cardNum]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many applications data rarely comes from text files, that being a fairly inefficient way to store large amounts of data. SQL-based relational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called NoSQL) databases have become quite popular. The choice of database is usually de- pendent on the performance, data integrity, and scalability needs of an application.\n",
    "\n",
    "Loading data from SQL into a DataFrame is fairly straightforward, and pandas has some functions to simplify the process. As an example, I’ll use an in-memory SQLite database using Python’s built-in sqlite3 driver. Here’s a short Python program that selects latitudes and longitudes from an SQLite database stored in a file called survey.db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: Site",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b436631cd8c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survey.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT Site.lat, Site.long FROM Site;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: Site"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"survey.db\")\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT Site.lat, Site.long FROM Site;\")\n",
    "results = cursor.fetchall()\n",
    "for r in results:\n",
    "    print(r)\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program starts by importing the sqlite3 library. If we were connecting to MySQL, DB2, or some other database, we would import a different library, but all of them provide the same functions, so that the rest of our program does not have to change (at least, not much) if we switch from one database to another.\n",
    "\n",
    "Line 2 establishes a connection to the database. Since we’re using SQLite, all we need to specify is the name of the database file. Other systems may require us to provide a username and password as well. Line 3 then uses this connection to create a cursor. Just like the cursor in an editor, its role is to keep track of where we are in the database.\n",
    "\n",
    "On line 4, we use that cursor to ask the database to execute a query for us. The query is written in SQL, and passed to cursor.execute as a string. It’s our job to make sure that SQL is properly formatted; if it isn’t, or if something goes wrong when it is being executed, the database will report an error.\n",
    "\n",
    "The database returns the results of the query to us in response to the cursor.fetchall call on line 5. This result is a list with one entry for each record in the result set; if we loop over that list (line 6) and print those list entries (line 7), we can see that each one is a tuple with one element for each field we asked for.\n",
    "\n",
    "Finally, lines 8 and 9 close our cursor and our connection, since the database can only keep a limited number of these open at one time. Since establishing a connection takes time, though, we shouldn’t open a connection, do one operation, then close the connection, only to reopen it a few microseconds later to do another operation. Instead, it’s normal to create one connection that stays open for the lifetime of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "Read your five different types of your own files to Python from your hard disk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
