{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新冠病毒网页抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework of week7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浩嬢嬢捉“谣”记㉛|无症状感染者是新冠病毒后期特征？\u00012020-04-17\u0001四川新闻网\u0001  四川新闻网特别推出系列微动画 盘点网络流行的疫情谣言 揭露谣言“嘴脸” 今日推出第三十一期 让我们转发起来 共同呼吁大家不信谣、不传谣、不造谣 携手打赢疫情防控攻坚战   \u0001https://k.sina.com.cn/article_1697698951_m6530d48703300uwyg.html?from=news&subch=onews\n",
      "报告：盖茨成新冠阴谋论头号靶子 被称制造了该病毒\u00012020-04-17\u0001cnBeta\u0001原标题：报告：盖茨成新冠阴谋论头号靶子 被称制造了该病毒    来源：新浪科技  《纽约时报》和分析公司Zignal Labs汇编的数据显示 新冠病毒疫情爆发后\u0001https://tech.sina.com.cn/roll/2020-04-17/doc-iirczymi6919845.shtml\n",
      "3名宇航员重返地球 迎接团队全部通过新冠病毒检测\u00012020-04-17\u0001新浪新闻综合\u0001由于三名宇航员返回的时间正值新冠病毒流行期间 所以地面工作人员需要采取额外的防护措施 所有迎接团队的成员都需要做必要的防护措施\u0001https://video.sina.com.cn/p/news/2020-04-17/detail-iircuyvh8382563.d.html\n",
      "45分钟出结果？新冠病毒核酸检测原来是这样！\u00012020-04-17\u0001新华网\u0001 45分钟给出病毒检测结果 体积较小、方便携带……近日 记者来到中科院苏州医工所 现场体验新冠病毒核酸快速检测系统 据悉\u0001https://k.sina.com.cn/article_2810373291_ma782e4ab03301nx1w.html?from=science\n",
      "新冠病毒绝非生物战，美军公布调查结果，称可能来源大自然\u00012020-04-17\u0001UTV兵鉴\u0001 新冠病毒绝非生物战 美军公布调查结果 称可能来源大自然  \u0001https://k.sina.com.cn/article_5381630750_m140c52b1e03300mvw2.html?from=mil\n",
      "3名宇航员重返地球 迎接团队全部通过新冠病毒检测\u00012020-04-17\u0001时间最现场\u0001由于三名宇航员返回的时间正值新冠病毒流行期间 所以地面工作人员需要采取额外的防护措施 所有迎接团队的成员都需要做必要的防护措施 还在之前都进行了新型冠状病毒的检测\u0001https://k.sina.com.cn/article_6300853659_m1778f659b03300pto1.html?from=science\n",
      "生命的奇迹！美国孕妇感染新冠病毒 戴呼吸器昏迷中顺利产子\u00012020-04-17\u0001时间国际视频\u0001索里亚诺在离预产期只有一个月的时候被确诊新冠肺炎 住院后 医生给她使用呼吸机 但她身体状况急剧恶化 考虑到母亲和孩子的安全 最终 医生决定在她昏睡状态下 对其进行剖腹产手术 产后 索里亚诺又进行了11天的治疗\u0001https://k.sina.com.cn/article_6349943205_m17a7c71a503300n0ra.html?from=baby\n",
      "感染新冠后，一辈子都会带病毒？这14个传言，脑洞真大\u00012020-04-17\u0001深圳卫健委\u0001    世界卫生组织曾警告 有关新冠病毒的错误信息会加大医卫人员的工作难度 并向公众传播恐惧、引起混乱 针对新近出现的一些传言 世卫组织近日在官网更新了辟谣性内容 请看下图⬇️ ​      \u0001https://k.sina.com.cn/article_2831150640_pa8bfee3002700pmk1.html?from=health\n",
      "新冠病毒已经暴发了19次所以叫COVID-19？白宫高级顾问彻底惹怒崔娃\u00012020-04-17\u0001海外网\u0001 新冠病毒已经暴发了19次所以叫COVID-19？白宫高级顾问彻底惹怒崔娃  \u0001https://k.sina.com.cn/article_3057540037_mb63e5bc505300qpz6.html?from=news&subch=onews\n",
      "海特生物：子公司新冠病毒检测试剂盒通过欧盟CE符合性声明\u00012020-04-17\u0001e公司\u0001�司海泰生物于近日取得两项医疗器械产品欧盟CE符合性声明文件 涉及产品为新型冠状病毒（2019-nCoV）ORF1ab/N基因/甲型流感/乙型流感检测试剂盒（荧光PCR法）、新型冠状病毒（2019-nCoV）ORF1ab/N基因检测试剂盒（荧光PCR法） 目前两个产品尚未取得中国医疗器械产品注册��\u0001https://k.sina.com.cn/article_5597884738_14da8f14202000zvnm.html?from=finance\n",
      "美国孕妇感染新冠病毒 戴呼吸器昏迷中顺利产子\u00012020-04-17\u0001时间国际视频\u0001  美国孕妇感染新冠病毒 戴呼吸器昏迷中顺利产子  \u0001https://k.sina.com.cn/article_6349943205_m17a7c71a505300n0r4.html?from=health\n",
      "库里谈到去检测新冠病毒的那一次经历说道~当时联盟一直都在说新冠对\u00012020-04-17\u0001体育章鱼哥\u0001    库里谈到去检测新冠病毒的那一次经历说道~当时联盟一直都在说新冠对于联盟的影响！不过在3月6号比赛结束之后 我就感觉到了不适\u0001https://k.sina.com.cn/article_3567974090_pd4aaf6ca02700nhnj.html?from=sports&subch=osport\n",
      "美最新研究发现：祛除疫情需群体免疫82%，新冠病毒传染力被低估\u00012020-04-17\u0001财经杂志\u0001　　原标题：美最新研究发现：祛除疫情需群体免疫82% 新冠病毒传染力被低估   　　据美国媒体报道 美国洛斯阿拉莫斯国家实验室（Los Alamos National\u0001https://news.sina.com.cn/w/2020-04-17/doc-iirczymi6901672.shtml\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "#每隔一段时间抓取新闻，并记录日志\n",
    "import requests\n",
    "import sys\n",
    "#与计算机对话\n",
    "import urllib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# page——加入此变量，可通过循环实现对所有页面的抓取\n",
    "#comp——字符串，内容\n",
    "def get_list(comp, page):\n",
    "    \"\"\"Function to get  web list pages for a given company and page number.\n",
    "\n",
    "    Args:\n",
    "        comp: Company name.\n",
    "        page: The page number.\n",
    "\n",
    "    Returns:\n",
    "        newsData: A dictionary with news title as its key and other details as values.\n",
    "\n",
    "    \"\"\"\n",
    "    #\"\"\"内为函数说明：帮自己明确自己的函数的功用\n",
    "    newsData = OrderedDict()\n",
    "    #存储在有序排列的字典里\n",
    "    href = 'http://search.sina.com.cn/?%s&range=title&c=news&num=20&col=1_7&page=%s' % (comp, page) # comp -> first %s; page -> 2nd %s; col=1_7 -> financial news in sina\n",
    "    #？q—%s; page=2—page=%s    ——加入变量\n",
    "    html = requests.get(href)\n",
    "    #请求网页内容\n",
    "    # Parsing html\n",
    "    soup = BeautifulSoup(html.content, 'html.parser') #第一节的内容\n",
    "    divs = soup.findAll('div', {\"class\": \"r-info r-info2\"})\n",
    "    for div in divs:\n",
    "        head = div.findAll('h2')[0]\n",
    "        # News title\n",
    "        titleinfo = head.find('a')\n",
    "        title = titleinfo.get_text()\n",
    "        # News url\n",
    "        url = titleinfo['href']\n",
    "        # Other info\n",
    "        otherinfo = head.find('span', {\"class\": \"fgray_time\"}).get_text()\n",
    "        source, date, time = otherinfo.split()\n",
    "        #split——数据拆分\n",
    "        # News abstract\n",
    "        abstract = div.find('p', {\"class\": \"content\"}).get_text()\n",
    "        newsData[title] = [date, source, abstract, url]\n",
    "    return newsData\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Python3标准格式：把以前的所有定义过的函数都引用过来（比如get_list）\n",
    "    compRawStr = '新冠病毒'\n",
    "    # Dealing with character encoding\n",
    "    comp = compRawStr.encode('gbk')\n",
    "    #上为把百度变成gbk编码（网页编码：gbk； python编码：utf-8）\n",
    "    d = {'q': comp}\n",
    "    pname = urlencode(d)\n",
    "    # Scraping and printing the first two pages\n",
    "    for page in range(6)[1:]:#range（3）=0,1,2,3——+[1:]=1,2,3\n",
    "        newsData = get_list(pname, page)\n",
    "        for ky in newsData:\n",
    "            print('\\001'.join([ky] + newsData[ky])) # \"\\001\" as separator分隔符——gbk中显示小方格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not collections.OrderedDict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9f244e2560e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CoronaVirus.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewsData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not collections.OrderedDict"
     ]
    }
   ],
   "source": [
    "with open(\"CoronaVirus.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(newsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xff in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-200d8634f261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Getting and printing content for each url in the crawled web list pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\wendy.wu\\\\Desktop\\\\数据科学工具\\\\python网络数据抓取\\\\xinguan_list.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\001'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Printing progress onto console\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xff in position 0: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_body(href):\n",
    "    \"\"\"Function to retrieve news content given its url.\n",
    "\n",
    "    Args:\n",
    "        href: url of the news to be crawled.\n",
    "\n",
    "    Returns:\n",
    "        content: the crawled news content.\n",
    "\n",
    "    \"\"\"\n",
    "    html = requests.get(href)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    div = soup.find('div', {\"id\": \"artibody\"})\n",
    "    paras = div.findAll('p')\n",
    "    content = ''\n",
    "    for p in paras:\n",
    "        ptext = p.get_text().strip().replace(\"\\n\", \"\")\n",
    "        content += ptext\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    # Getting and printing content for each url in the crawled web list pages\n",
    "    with open(\"CoronaVirus.txt\") as f:\n",
    "        for line in f:\n",
    "            title, date, source, abstract, href = line.strip().split('\\001')\n",
    "            # Printing progress onto console\n",
    "            logging.info('Scraping ' + href)\n",
    "            content = get_body(href)\n",
    "            print('\\001'.join([title, date, source, abstract, href, content]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
